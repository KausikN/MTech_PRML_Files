{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJO6QJk3Tn2G"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:05.780190Z",
          "iopub.execute_input": "2021-11-12T17:35:05.780932Z",
          "iopub.status.idle": "2021-11-12T17:35:05.785426Z",
          "shell.execute_reply.started": "2021-11-12T17:35:05.780891Z",
          "shell.execute_reply": "2021-11-12T17:35:05.784828Z"
        },
        "trusted": true,
        "id": "XJoGQS01Tn2L"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns5IbqeqTn2M"
      },
      "source": [
        "# Dataset Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:06.123789Z",
          "iopub.execute_input": "2021-11-12T17:35:06.124315Z",
          "iopub.status.idle": "2021-11-12T17:35:06.140003Z",
          "shell.execute_reply.started": "2021-11-12T17:35:06.124272Z",
          "shell.execute_reply": "2021-11-12T17:35:06.139150Z"
        },
        "trusted": true,
        "id": "Frx-nvYzTn2N"
      },
      "source": [
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Main Functions\n",
        "# Dataset Functions\n",
        "def ReadDataset(path):\n",
        "    dataset = pd.read_csv(path)\n",
        "    return dataset\n",
        "\n",
        "def SaveDataset(dataset, path):\n",
        "    dataset.to_csv(path, index=False)\n",
        "\n",
        "def FormatDataset(dataset, targetRowCount=0):\n",
        "    data_pts = []\n",
        "    targets = []\n",
        "    if targetRowCount == 0:\n",
        "        data_pts = dataset.iloc[:, 1:].values.T\n",
        "        targets = np.array([])\n",
        "    else:\n",
        "        data_pts = dataset.iloc[:-targetRowCount, 1:].values.T\n",
        "        targets = dataset.iloc[-targetRowCount:, 1:].values\n",
        "\n",
        "    print(\"Data points: \", data_pts.shape)\n",
        "    print(\"Targets: \", targets.shape)\n",
        "\n",
        "    return data_pts, targets\n",
        "\n",
        "# Specific Functions\n",
        "def LoadContestDatasets(train_dataset_path_1, test_dataset_path_1, train_dataset_path_2, test_dataset_path_2):\n",
        "    # Dataset 1\n",
        "    print(\"Train Dataset 1\")\n",
        "    train_dataset_1 = ReadDataset(train_dataset_path_1)\n",
        "    train_data_pts_1, train_targets_1 = FormatDataset(train_dataset_1, targetRowCount=2)\n",
        "    print(\"Test Dataset 1\")\n",
        "    test_dataset_1 = ReadDataset(test_dataset_path_1)\n",
        "    test_data_pts_1, _ = FormatDataset(test_dataset_1, targetRowCount=0)\n",
        "\n",
        "    # Dataset 2\n",
        "    print(\"Train Dataset 2\")\n",
        "    train_dataset_2 = ReadDataset(train_dataset_path_2)\n",
        "    train_data_pts_2, train_targets_2 = FormatDataset(train_dataset_2, targetRowCount=4)\n",
        "    print(\"Test Dataset 2\")\n",
        "    test_dataset_2 = ReadDataset(test_dataset_path_2)\n",
        "    test_data_pts_2, _ = FormatDataset(test_dataset_2, targetRowCount=0)\n",
        "\n",
        "    return train_data_pts_1, train_targets_1, test_data_pts_1, train_data_pts_2, train_targets_2, test_data_pts_2\n",
        "\n",
        "def SaveContestPredictions(Predictions, preds_save_path):\n",
        "    # Append all predictions to single list\n",
        "    Predictions_Appended = []\n",
        "    for preds in Predictions:\n",
        "        Predictions_Appended.extend(preds)\n",
        "    Predictions_Appended = [int(round(pred, 0)) for pred in Predictions_Appended]\n",
        "    print(\"Predictions Count:\", len(Predictions_Appended))\n",
        "\n",
        "    # Form a dataframe\n",
        "    indices = list(range(len(Predictions_Appended)))\n",
        "    preds_data = np.dstack((indices, Predictions_Appended))[0]\n",
        "    Predictions_df = pd.DataFrame(data=preds_data, columns=[\"Id\", \"Predicted\"])\n",
        "    SaveDataset(Predictions_df, preds_save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:06.297691Z",
          "iopub.execute_input": "2021-11-12T17:35:06.298629Z",
          "iopub.status.idle": "2021-11-12T17:35:06.302535Z",
          "shell.execute_reply.started": "2021-11-12T17:35:06.298579Z",
          "shell.execute_reply": "2021-11-12T17:35:06.301931Z"
        },
        "trusted": true,
        "id": "D0AiLtFiTn2O"
      },
      "source": [
        "# Dataset Paths\n",
        "train_dataset_path_1 = \"./Dataset_1_Training.csv\"\n",
        "test_dataset_path_1 = \"./Dataset_1_Testing.csv\"\n",
        "train_dataset_path_2 = \".Dataset_2_Training.csv\"\n",
        "test_dataset_path_2 = \"./Dataset_2_Testing.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:06.461923Z",
          "iopub.execute_input": "2021-11-12T17:35:06.462809Z",
          "iopub.status.idle": "2021-11-12T17:35:12.848004Z",
          "shell.execute_reply.started": "2021-11-12T17:35:06.462764Z",
          "shell.execute_reply": "2021-11-12T17:35:12.847030Z"
        },
        "trusted": true,
        "id": "Cp-x2YpFTn2P"
      },
      "source": [
        "# Load Datasets\n",
        "train_data_pts_1_original, train_targets_1, test_data_pts_1_original, train_data_pts_2_original, train_targets_2, test_data_pts_2_original = \\\n",
        "    LoadContestDatasets(train_dataset_path_1, test_dataset_path_1, train_dataset_path_2, test_dataset_path_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKDqmf2-Tn2P"
      },
      "source": [
        "# Dataset Visualisations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:12.850339Z",
          "iopub.execute_input": "2021-11-12T17:35:12.850555Z",
          "iopub.status.idle": "2021-11-12T17:35:12.865373Z",
          "shell.execute_reply.started": "2021-11-12T17:35:12.850529Z",
          "shell.execute_reply": "2021-11-12T17:35:12.864382Z"
        },
        "trusted": true,
        "id": "tvgJwGmbTn2Q"
      },
      "source": [
        "# Visualisation Functions\n",
        "def PlotDatasetCovariance(data_pts, title=\"\"):\n",
        "    # Visualise the covariance\n",
        "    cov_matrix = np.cov(data_pts)\n",
        "    plt.imshow(cov_matrix, cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def PlotDatasetMean(data_pts, title=\"\"):\n",
        "    # Visualise the mean\n",
        "    mean_data_pt = np.mean(data_pts, axis=0)\n",
        "    size = mean_data_pt.shape[0]\n",
        "\n",
        "    # Plot\n",
        "    plt.plot(list(range(size)), mean_data_pt)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def PlotDatasetVariance(data_pts, title=\"\"):\n",
        "    # Visualise the variance\n",
        "    variance = np.var(data_pts, axis=0)\n",
        "    size = variance.shape[0]\n",
        "\n",
        "    # Plot\n",
        "    plt.plot(list(range(size)), variance)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    \n",
        "def ShowClassImbalanceDataset(targets, title=''):\n",
        "    # Calculate the class imbalance\n",
        "    targets = np.array(targets, dtype=int)\n",
        "    class_counts = np.bincount(targets)\n",
        "    # class_counts = class_counts[1:]\n",
        "    # class_counts = class_counts / np.sum(class_counts)\n",
        "\n",
        "    # Show the class imbalance\n",
        "    plt.bar(list(range(len(class_counts))), class_counts)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    \n",
        "def PlotDatasetMeanBins(data_pts, bins=100, title=\"\"):\n",
        "    # Visualise the mean\n",
        "    mean = np.mean(data_pts, axis=0)\n",
        "    size = mean.shape[0]\n",
        "\n",
        "    # Plot\n",
        "    plt.hist(mean, bins=bins)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def PlotDatasetVarianceBins(data_pts, bins=100, title=\"\"):\n",
        "    # Visualise the variance\n",
        "    variance = np.var(data_pts, axis=0)\n",
        "    size = variance.shape[0]\n",
        "    \n",
        "    # Plot\n",
        "    plt.hist(variance, bins=bins)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:12.866721Z",
          "iopub.execute_input": "2021-11-12T17:35:12.867022Z",
          "iopub.status.idle": "2021-11-12T17:35:16.279215Z",
          "shell.execute_reply.started": "2021-11-12T17:35:12.866990Z",
          "shell.execute_reply": "2021-11-12T17:35:16.278133Z"
        },
        "trusted": true,
        "id": "LBZsHbWeTn2Q"
      },
      "source": [
        "print(\"TRAIN DATA VISUALISATION - ORIGINAL\")\n",
        "# Train Dataset 1\n",
        "# Visualise Covariance\n",
        "PlotDatasetCovariance(train_data_pts_1_original, \"Original Train Dataset 1 Covariance\")\n",
        "\n",
        "# Visualise Mean\n",
        "PlotDatasetMean(train_data_pts_1_original, \"Original Train Dataset 1 Mean\")\n",
        "PlotDatasetMeanBins(train_data_pts_1_original, 100, \"Original Train Dataset 1 Mean Histogram\")\n",
        "\n",
        "# Visualise Variance\n",
        "PlotDatasetVariance(train_data_pts_1_original, \"Original Train Dataset 1 Variance\")\n",
        "PlotDatasetVarianceBins(train_data_pts_1_original, 100, \"Original Train Dataset 1 Variance Histogram\")\n",
        "\n",
        "# Train Dataset 2\n",
        "# Visualise Covariance\n",
        "PlotDatasetCovariance(train_data_pts_2_original, \"Original Train Dataset 2 Covariance\")\n",
        "\n",
        "# Visualise Mean\n",
        "PlotDatasetMean(train_data_pts_2_original, \"Original Train Dataset 2 Mean\")\n",
        "PlotDatasetMeanBins(train_data_pts_2_original, 100, \"Original Train Dataset 2 Mean Histogram\")\n",
        "\n",
        "# Visualise Variance\n",
        "PlotDatasetVariance(train_data_pts_2_original, \"Original Train Dataset 2 Variance\")\n",
        "PlotDatasetVarianceBins(train_data_pts_2_original, 100, \"Original Train Dataset 2 Variance Histogram\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:16.282012Z",
          "iopub.execute_input": "2021-11-12T17:35:16.282440Z",
          "iopub.status.idle": "2021-11-12T17:35:19.870881Z",
          "shell.execute_reply.started": "2021-11-12T17:35:16.282392Z",
          "shell.execute_reply": "2021-11-12T17:35:19.870124Z"
        },
        "trusted": true,
        "id": "x5xEG5YBTn2R"
      },
      "source": [
        "print(\"TEST DATA VISUALISATION - ORIGINAL\")\n",
        "# Test Dataset 1\n",
        "# Visualise Covariance\n",
        "PlotDatasetCovariance(test_data_pts_1_original, \"Original Test Dataset 1 Covariance\")\n",
        "\n",
        "# Visualise Mean\n",
        "PlotDatasetMean(test_data_pts_1_original, \"Original Test Dataset 1 Mean\")\n",
        "PlotDatasetMeanBins(test_data_pts_1_original, 100, \"Original Test Dataset 1 Mean Histogram\")\n",
        "\n",
        "# Visualise Variance\n",
        "PlotDatasetVariance(test_data_pts_1_original, \"Original Test Dataset 1 Variance\")\n",
        "PlotDatasetVarianceBins(test_data_pts_1_original, 100, \"Original Test Dataset 1 Variance Histogram\")\n",
        "\n",
        "# Test Dataset 2\n",
        "# Visualise Covariance\n",
        "PlotDatasetCovariance(test_data_pts_2_original, \"Original Test Dataset 2 Covariance\")\n",
        "\n",
        "# Visualise Mean\n",
        "PlotDatasetMean(test_data_pts_2_original, \"Original Test Dataset 2 Mean\")\n",
        "PlotDatasetMeanBins(test_data_pts_2_original, 100, \"Original Test Dataset 2 Mean Histogram\")\n",
        "\n",
        "# Visualise Variance\n",
        "PlotDatasetVariance(test_data_pts_2_original, \"Original Test Dataset 2 Variance\")\n",
        "PlotDatasetVarianceBins(test_data_pts_2_original, 100, \"Original Test Dataset 2 Variance Histogram\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlFn1lf3Tn2S"
      },
      "source": [
        "# Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:19.872139Z",
          "iopub.execute_input": "2021-11-12T17:35:19.872363Z",
          "iopub.status.idle": "2021-11-12T17:35:20.309997Z",
          "shell.execute_reply.started": "2021-11-12T17:35:19.872335Z",
          "shell.execute_reply": "2021-11-12T17:35:20.309081Z"
        },
        "trusted": true,
        "id": "O_faybB5Tn2S"
      },
      "source": [
        "train_data_pts_features = []\n",
        "test_data_pts_features = []\n",
        "\n",
        "train_targets = list(train_targets_1)\n",
        "train_targets.extend(train_targets_2)\n",
        "\n",
        "for i in range(2):\n",
        "    train_data_pts_features.append(np.copy(train_data_pts_1_original))\n",
        "    test_data_pts_features.append(np.copy(test_data_pts_1_original))\n",
        "for i in range(4):\n",
        "    train_data_pts_features.append(np.copy(train_data_pts_2_original))\n",
        "    test_data_pts_features.append(np.copy(test_data_pts_2_original))\n",
        "\n",
        "# Print Dataset Shapes\n",
        "print(\"TRAIN DATASETS: \")\n",
        "for i in range(len(train_data_pts_features)):\n",
        "    print(\"CO_\" + str(i+1) + \" Data points: \", train_data_pts_features[i].shape)\n",
        "\n",
        "print(\"TEST DATASETS: \")\n",
        "for i in range(len(test_data_pts_features)):\n",
        "    print(\"CO_\" + str(i+1) + \" Data points: \", test_data_pts_features[i].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:20.311866Z",
          "iopub.execute_input": "2021-11-12T17:35:20.312200Z",
          "iopub.status.idle": "2021-11-12T17:35:20.319633Z",
          "shell.execute_reply.started": "2021-11-12T17:35:20.312156Z",
          "shell.execute_reply": "2021-11-12T17:35:20.318559Z"
        },
        "trusted": true,
        "id": "Ym9sEQnUTn2S"
      },
      "source": [
        "# Imports\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Preprocessing Functions\n",
        "def MinMaxScaleDataset(dataset_pts):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled = scaler.fit_transform(dataset_pts)\n",
        "    return scaled\n",
        "\n",
        "def StandardScaleDataset(dataset_pts):\n",
        "    scaler = StandardScaler()\n",
        "    scaled = scaler.fit_transform(dataset_pts)\n",
        "    return scaled\n",
        "\n",
        "def CombinedStandardScaleDataset(datasets_pts):\n",
        "    scaler = StandardScaler()\n",
        "    scaled_train = scaler.fit_transform(datasets_pts[0])\n",
        "    scaled_test = scaler.transform(datasets_pts[1])\n",
        "    return scaled_train, scaled_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:20.321764Z",
          "iopub.execute_input": "2021-11-12T17:35:20.322111Z",
          "iopub.status.idle": "2021-11-12T17:35:23.266334Z",
          "shell.execute_reply.started": "2021-11-12T17:35:20.322066Z",
          "shell.execute_reply": "2021-11-12T17:35:23.265359Z"
        },
        "trusted": true,
        "id": "0SrruQW1Tn2T"
      },
      "source": [
        "train_data_pts = []\n",
        "test_data_pts = []\n",
        "\n",
        "# # Original Datasets\n",
        "# for pts in train_data_pts_features:\n",
        "#     train_data_pts.append(np.copy(pts))\n",
        "# for pts in test_data_pts_features:\n",
        "#     test_data_pts.append(np.copy(pts))\n",
        "\n",
        "# # MinMaxScale Datasets Separately\n",
        "# for pts in train_data_pts_features:\n",
        "#     pts_scaled = MinMaxScaleDataset(pts)\n",
        "#     train_data_pts.append(pts_scaled)\n",
        "# for pts in test_data_pts_features:\n",
        "#     pts_scaled = MinMaxScaleDataset(pts)\n",
        "#     test_data_pts.append(pts_scaled)\n",
        "    \n",
        "# StandardScale Datasets Separately\n",
        "for pts in train_data_pts_features:\n",
        "    pts_scaled = StandardScaleDataset(pts)\n",
        "    train_data_pts.append(pts_scaled)\n",
        "for pts in test_data_pts_features:\n",
        "    pts_scaled = StandardScaleDataset(pts)\n",
        "    test_data_pts.append(pts_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnBrbivxTn2T"
      },
      "source": [
        "# Preprocessed Dataset Visualisations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:23.267742Z",
          "iopub.execute_input": "2021-11-12T17:35:23.267990Z",
          "iopub.status.idle": "2021-11-12T17:35:28.321500Z",
          "shell.execute_reply.started": "2021-11-12T17:35:23.267959Z",
          "shell.execute_reply": "2021-11-12T17:35:28.320609Z"
        },
        "trusted": true,
        "id": "jfMCskYBTn2T"
      },
      "source": [
        "print(\"TRAIN DATA CLASSES\")\n",
        "# Visualise Class Imbalance\n",
        "co = 0\n",
        "for i in range(train_targets_1.shape[0]):\n",
        "    co += 1\n",
        "    ShowClassImbalanceDataset(train_targets_1[i], \"Train Dataset 1 CO_\" + str(co) + \" Classes\")\n",
        "for i in range(train_targets_2.shape[0]):\n",
        "    co += 1\n",
        "    ShowClassImbalanceDataset(train_targets_2[i], \"Train Dataset 2 CO_\" + str(co) + \" Classes\")\n",
        "\n",
        "print(\"TRAIN DATA VISUALISATION - SCALED\")\n",
        "# Train Dataset 1\n",
        "# Visualise Covariance\n",
        "PlotDatasetCovariance(train_data_pts[0], \"Train Dataset 1 Covariance\")\n",
        "\n",
        "# Visualise Mean\n",
        "PlotDatasetMean(train_data_pts[0], \"Train Dataset 1 Mean\")\n",
        "PlotDatasetMeanBins(train_data_pts[0], 100, \"Train Dataset 1 Mean Histogram\")\n",
        "\n",
        "# Visualise Variance\n",
        "PlotDatasetVariance(train_data_pts[0], \"Train Dataset 1 Variance\")\n",
        "PlotDatasetVarianceBins(train_data_pts[0], 100, \"Train Dataset 1 Variance Histogram\")\n",
        "\n",
        "# Train Dataset 2\n",
        "# Visualise Covariance\n",
        "PlotDatasetCovariance(train_data_pts[-1], \"Train Dataset 2 Covariance\")\n",
        "\n",
        "# Visualise Mean\n",
        "PlotDatasetMean(train_data_pts[-1], \"Train Dataset 2 Mean\")\n",
        "PlotDatasetMeanBins(train_data_pts[-1], 100, \"Train Dataset 2 Mean Histogram\")\n",
        "\n",
        "# Visualise Variance\n",
        "PlotDatasetVariance(train_data_pts[-1], \"Train Dataset 2 Variance\")\n",
        "PlotDatasetVarianceBins(train_data_pts[-1], 100, \"Train Dataset 2 Variance Histogram\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:28.322781Z",
          "iopub.execute_input": "2021-11-12T17:35:28.323027Z",
          "iopub.status.idle": "2021-11-12T17:35:31.659637Z",
          "shell.execute_reply.started": "2021-11-12T17:35:28.323001Z",
          "shell.execute_reply": "2021-11-12T17:35:31.658871Z"
        },
        "trusted": true,
        "id": "mBz2b5suTn2U"
      },
      "source": [
        "print(\"TEST DATA VISUALISATION - SCALED\")\n",
        "# Test Dataset 1\n",
        "# Visualise Covariance\n",
        "PlotDatasetCovariance(test_data_pts[0], \"Test Dataset 1 Covariance\")\n",
        "\n",
        "# Visualise Mean\n",
        "PlotDatasetMean(test_data_pts[0], \"Test Dataset 1 Mean\")\n",
        "PlotDatasetMeanBins(test_data_pts[0], 100, \"Test Dataset 1 Mean Histogram\")\n",
        "\n",
        "# Visualise Variance\n",
        "PlotDatasetVariance(test_data_pts[0], \"Test Dataset 1 Variance\")\n",
        "PlotDatasetVarianceBins(test_data_pts[0], 100, \"Test Dataset 1 Variance Histogram\")\n",
        "\n",
        "# Test Dataset 2\n",
        "# Visualise Covariance\n",
        "PlotDatasetCovariance(test_data_pts[-1], \"Test Dataset 2 Covariance\")\n",
        "\n",
        "# Visualise Mean\n",
        "PlotDatasetMean(test_data_pts[-1], \"Test Dataset 2 Mean\")\n",
        "PlotDatasetMeanBins(test_data_pts[-1], 100, \"Test Dataset 2 Mean Histogram\")\n",
        "\n",
        "# Visualise Variance\n",
        "PlotDatasetVariance(test_data_pts[-1], \"Test Dataset 2 Variance\")\n",
        "PlotDatasetVarianceBins(test_data_pts[-1], 100, \"Test Dataset 2 Variance Histogram\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW8dRth1Tn2U"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ2Lu5H6Tn2U"
      },
      "source": [
        "## AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:31.662081Z",
          "iopub.execute_input": "2021-11-12T17:35:31.662315Z",
          "iopub.status.idle": "2021-11-12T17:35:31.670631Z",
          "shell.execute_reply.started": "2021-11-12T17:35:31.662289Z",
          "shell.execute_reply": "2021-11-12T17:35:31.669826Z"
        },
        "trusted": true,
        "id": "FcScCLwKTn2V"
      },
      "source": [
        "# Imports\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Main Functions\n",
        "# AdaBoost Functions\n",
        "def AdaBoost_Run(train_data_pts, train_targets, colsample_bytree=0.3, learning_rate=0.1, max_depth=10, alpha=10, n_estimators=200, seed=0):\n",
        "    X_train, X_test, y_train, y_test = train_data_pts, train_data_pts, train_targets, train_targets\n",
        "\n",
        "    classifier = AdaBoostClassifier(\n",
        "#         RandomForestClassifier( n_estimators=5,\n",
        "        DecisionTreeClassifier(\n",
        "            max_depth=int(max_depth), random_state=int(seed)\n",
        "        ), \n",
        "        n_estimators=int(n_estimators), learning_rate=learning_rate, random_state=int(seed))\n",
        "    trained_classifier = classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Print Results\n",
        "    y_pred = trained_classifier.predict(X_test)\n",
        "    missclassified_count = (y_test != y_pred).sum()\n",
        "    print(\"Missclassified:\", missclassified_count, \"/\", X_test.shape[0])\n",
        "    print(\"\\n\")\n",
        "        \n",
        "    return trained_classifier, missclassified_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQfCUPwTTn2V"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:31.672163Z",
          "iopub.execute_input": "2021-11-12T17:35:31.672374Z",
          "iopub.status.idle": "2021-11-12T17:35:31.693313Z",
          "shell.execute_reply.started": "2021-11-12T17:35:31.672348Z",
          "shell.execute_reply": "2021-11-12T17:35:31.692596Z"
        },
        "trusted": true,
        "id": "2En1PrrWTn2V"
      },
      "source": [
        "def RunContestTraining(Xs, Ys, \n",
        "                       params):\n",
        "    ALGO = params[0]\n",
        "    colsample_bytree=params[1]\n",
        "    learning_rate=params[2]\n",
        "    max_depth=params[3]\n",
        "    alpha=params[4]\n",
        "    n_estimators=params[5]\n",
        "    seed=params[6]\n",
        "    trained_classifiers = []\n",
        "    missclassified_counts = []\n",
        "    curCOs = []\n",
        "\n",
        "    #### DATASET 1 ####\n",
        "    # Train Classifier\n",
        "    print(\"DATASET 1\")\n",
        "\n",
        "    # CO_1\n",
        "    curCO = 1\n",
        "    print(\"For C\" + str(curCO) + \": \")\n",
        "    trained_classifier, missclassified_count = ALGO[0](Xs[0], Ys[0], # MODEL CHANGE\n",
        "        colsample_bytree=colsample_bytree[0], learning_rate=learning_rate[0], max_depth=max_depth[0], \n",
        "                                      alpha=alpha[0], n_estimators=n_estimators[0], seed=seed[0])\n",
        "    trained_classifiers.append(trained_classifier)\n",
        "    missclassified_counts.append(missclassified_count)\n",
        "    curCOs.append(curCO)\n",
        "    print()\n",
        "\n",
        "    # CO_2\n",
        "    curCO = 2\n",
        "    print(\"For C\" + str(curCO) + \": \")\n",
        "    trained_classifier, missclassified_count = ALGO[1](Xs[1], Ys[1], # MODEL CHANGE\n",
        "        colsample_bytree=colsample_bytree[1], learning_rate=learning_rate[1], max_depth=max_depth[1], \n",
        "                                      alpha=alpha[1], n_estimators=n_estimators[1], seed=seed[1])\n",
        "    trained_classifiers.append(trained_classifier)\n",
        "    missclassified_counts.append(missclassified_count)\n",
        "    curCOs.append(curCO)\n",
        "    print()\n",
        "\n",
        "\n",
        "    #### DATASET 2 ####\n",
        "    # Train Classifier\n",
        "    print(\"DATASET 2\")\n",
        "\n",
        "    # CO_3\n",
        "    curCO = 3\n",
        "    print(\"For C\" + str(curCO) + \": \")\n",
        "    trained_classifier, missclassified_count = ALGO[2](Xs[2], Ys[2], # MODEL CHANGE\n",
        "        colsample_bytree=colsample_bytree[2], learning_rate=learning_rate[2], max_depth=max_depth[2], \n",
        "                                      alpha=alpha[2], n_estimators=n_estimators[2], seed=seed[2])\n",
        "    trained_classifiers.append(trained_classifier)\n",
        "    missclassified_counts.append(missclassified_count)\n",
        "    curCOs.append(curCO)\n",
        "    print()\n",
        "\n",
        "    # CO_4\n",
        "    curCO = 4\n",
        "    print(\"For C\" + str(curCO) + \": \")\n",
        "    trained_classifier, missclassified_count = ALGO[3](Xs[3], Ys[3], # MODEL CHANGE\n",
        "        colsample_bytree=colsample_bytree[3], learning_rate=learning_rate[3], max_depth=max_depth[3], \n",
        "                                      alpha=alpha[3], n_estimators=n_estimators[3], seed=seed[3])\n",
        "    trained_classifiers.append(trained_classifier)\n",
        "    missclassified_counts.append(missclassified_count)\n",
        "    curCOs.append(curCO)\n",
        "    print()\n",
        "\n",
        "    # CO_5\n",
        "    curCO = 5\n",
        "    print(\"For C\" + str(curCO) + \": \")\n",
        "    trained_classifier, missclassified_count = ALGO[4](Xs[4], Ys[4], # MODEL CHANGE\n",
        "        colsample_bytree=colsample_bytree[4], learning_rate=learning_rate[4], max_depth=max_depth[4], \n",
        "                                      alpha=alpha[4], n_estimators=n_estimators[4], seed=seed[4])\n",
        "    trained_classifiers.append(trained_classifier)\n",
        "    missclassified_counts.append(missclassified_count)\n",
        "    curCOs.append(curCO)\n",
        "    print()\n",
        "\n",
        "    # CO_6\n",
        "    curCO = 6\n",
        "    print(\"For C\" + str(curCO) + \": \")\n",
        "    trained_classifier, missclassified_count = ALGO[5](Xs[5], Ys[5], # MODEL CHANGE\n",
        "        colsample_bytree=colsample_bytree[5], learning_rate=learning_rate[5], max_depth=max_depth[5], \n",
        "                                      alpha=alpha[5], n_estimators=n_estimators[5], seed=seed[5])\n",
        "    trained_classifiers.append(trained_classifier)\n",
        "    missclassified_counts.append(missclassified_count)\n",
        "    curCOs.append(curCO)\n",
        "    print()\n",
        "    \n",
        "    return trained_classifiers, missclassified_counts, curCOs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:31.694438Z",
          "iopub.execute_input": "2021-11-12T17:35:31.694706Z",
          "iopub.status.idle": "2021-11-12T17:35:31.704913Z",
          "shell.execute_reply.started": "2021-11-12T17:35:31.694675Z",
          "shell.execute_reply": "2021-11-12T17:35:31.704106Z"
        },
        "trusted": true,
        "id": "1YAL8IB_Tn2V"
      },
      "source": [
        "def RunContestPredict(Xs, Ys, trained_classifiers, curCOs):\n",
        "    # Predict on Validation Dataset\n",
        "    missclassified_counts = []\n",
        "    for i in range(len(trained_classifiers)):\n",
        "        trained_classifier = trained_classifiers[i]\n",
        "        y_pred = trained_classifier.predict(Xs[i])\n",
        "        missclassified_count = (Ys[i] != y_pred).sum()\n",
        "        print(\"For C\" + str(curCOs[i]) + \": \")\n",
        "        print(\"Missclassified:\", missclassified_count, \"/\", Xs[i].shape[0])\n",
        "        print(\"\\n\")\n",
        "        missclassified_counts.append(missclassified_count)\n",
        "    \n",
        "    return missclassified_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VwSgkY3Tn2W"
      },
      "source": [
        "## HyperParameter Tuning and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:31.705929Z",
          "iopub.execute_input": "2021-11-12T17:35:31.706456Z",
          "iopub.status.idle": "2021-11-12T17:35:31.715515Z",
          "shell.execute_reply.started": "2021-11-12T17:35:31.706419Z",
          "shell.execute_reply": "2021-11-12T17:35:31.714986Z"
        },
        "trusted": true,
        "id": "cN0sBkYKTn2W"
      },
      "source": [
        "# HyperParamsTests = [\n",
        "#     [\n",
        "#         [AdaBoost_Run, 0.75, 0.2, 1, 10, 200, 0],\n",
        "#         [AdaBoost_Run, 0.75, 0.2, 1, 10, 200, 0],\n",
        "#         [AdaBoost_Run, 0.75, 0.2, 1, 10, 200, 0],\n",
        "#         [AdaBoost_Run, 0.75, 0.2, 1, 10, 200, 0],\n",
        "#         [AdaBoost_Run, 0.75, 0.25, 1, 10, 30, 0],\n",
        "#         [AdaBoost_Run, 0.75, 0.25, 1, 10, 30, 0],\n",
        "#     ]\n",
        "# ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:31.716795Z",
          "iopub.execute_input": "2021-11-12T17:35:31.717241Z",
          "iopub.status.idle": "2021-11-12T17:35:31.732796Z",
          "shell.execute_reply.started": "2021-11-12T17:35:31.717209Z",
          "shell.execute_reply": "2021-11-12T17:35:31.731938Z"
        },
        "trusted": true,
        "id": "LVoi6_HeTn2W"
      },
      "source": [
        "# # Dataset Split for Validation\n",
        "# X_trains = []\n",
        "# Y_trains = []\n",
        "# X_vals = []\n",
        "# Y_vals = []\n",
        "\n",
        "# for i in range(len(train_data_pts)):\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(train_data_pts[i], train_targets[i].T, \n",
        "#                 test_size=0.2, random_state=0) # SPLIT SEED -- CHANGE\n",
        "#     X_trains.append(X_train)\n",
        "#     Y_trains.append(y_train.T)\n",
        "#     X_vals.append(X_test)\n",
        "#     Y_vals.append(y_test.T)\n",
        "\n",
        "#     print(\"DATASET CO_\" + str(i+1))\n",
        "#     print(\"Train: X:\", X_train.shape, \"Y:\", y_train.shape)\n",
        "#     print(\"Val: X:\", X_test.shape, \"Y:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:31.734509Z",
          "iopub.execute_input": "2021-11-12T17:35:31.735193Z",
          "iopub.status.idle": "2021-11-12T17:35:31.744295Z",
          "shell.execute_reply.started": "2021-11-12T17:35:31.735151Z",
          "shell.execute_reply": "2021-11-12T17:35:31.743741Z"
        },
        "trusted": true,
        "id": "IMO1UaLQTn2W"
      },
      "source": [
        "# # Run Contest Training and Prediction on Grid of Hyperparams\n",
        "# train_errors = []\n",
        "# val_errors = []\n",
        "# runs_trained_classifiers = []\n",
        "# for i in tqdm(range(len(HyperParamsTests))):\n",
        "#     params = list(np.array(HyperParamsTests[i]).T)\n",
        "#     print(\"RUNNING TEST:\", i+1)\n",
        "#     print(\"PARAMS: colsample_bytree, learning_rate, max_depth, alpha, n_estimators, seed\")\n",
        "#     print(\"VALUES:\", params)\n",
        "#     print()\n",
        "#     print(\"Training...\")\n",
        "#     trained_classifiers, train_missclassified_counts, curCOs = RunContestTraining(X_trains, Y_trains, \n",
        "#                            params)\n",
        "#     print(\"Validating...\")\n",
        "#     val_missclassified_counts = RunContestPredict(X_vals, Y_vals, trained_classifiers, curCOs)\n",
        "# #     train_missclassified_counts = [0, 0, 0, 0, 0, 0]\n",
        "# #     val_missclassified_counts = [0, 0, 0, 0, 0, 0]\n",
        "# #     trained_classifiers = [None, None, None, None, None]\n",
        "#     train_errors.append(train_missclassified_counts)\n",
        "#     val_errors.append(val_missclassified_counts)\n",
        "#     runs_trained_classifiers.append(trained_classifiers)\n",
        "#     print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:31.745526Z",
          "iopub.execute_input": "2021-11-12T17:35:31.745932Z",
          "iopub.status.idle": "2021-11-12T17:35:31.757208Z",
          "shell.execute_reply.started": "2021-11-12T17:35:31.745902Z",
          "shell.execute_reply": "2021-11-12T17:35:31.756639Z"
        },
        "trusted": true,
        "id": "uVOi4Pt1Tn2X"
      },
      "source": [
        "# # Save Test Run Errors and Data\n",
        "# run_save_path = \"./run_data.csv\"\n",
        "\n",
        "# HyperParams = []\n",
        "# for hparams in HyperParamsTests:\n",
        "#     hparams = np.array(hparams).T\n",
        "#     hparams_joined = [\", \".join([f.__name__ for f in hparams[0]])]\n",
        "#     for p in hparams[1:]:\n",
        "#         hparams_joined.append(\", \".join(list(map(str, p))))\n",
        "#     HyperParams.append(hparams_joined)\n",
        "# HyperParams = np.array(HyperParams).T\n",
        "# HyperParamsColumnNames = [\"algo\", \"colsample_bytree\", \"learning_rate\", \"max_depth\", \"alpha\", \"n_estimators\", \"seed\"]\n",
        "\n",
        "# # Form a dataframe\n",
        "# train_n_pts = []\n",
        "# val_n_pts = []\n",
        "# for i in range(len(X_trains)):\n",
        "#     train_n_pts.append(X_trains[i].shape[0])\n",
        "#     val_n_pts.append(X_vals[i].shape[0])\n",
        "# train_n_pts_total = sum(train_n_pts)\n",
        "# val_n_pts_total = sum(val_n_pts)\n",
        "# train_n_pts_array = np.array(train_n_pts)\n",
        "# val_n_pts_array = np.array(val_n_pts)\n",
        "\n",
        "# total_train_errors_stacked = list((np.sum(train_errors, axis=1) / (train_n_pts_total)).T)\n",
        "# total_val_errors_stacked = list((np.sum(val_errors, axis=1) / (val_n_pts_total)).T)\n",
        "# train_errors_stacked = list((np.array(train_errors) / (train_n_pts_array)).T)\n",
        "# val_errors_stacked = list((np.array(val_errors) / (val_n_pts_array)).T)\n",
        "# indices = list(range(len(train_errors)))\n",
        "# columns_data = ([indices]\n",
        "#                 + list(HyperParams)\n",
        "#                 + [total_train_errors_stacked, total_val_errors_stacked]\n",
        "#                 + train_errors_stacked + val_errors_stacked)\n",
        "# # columns_data = np.array(columns_data)\n",
        "# columns_names = ([\"id\"] + HyperParamsColumnNames\n",
        "#                 + [\"total_train_error_\" + str(train_n_pts_total), \"total_val_error_\" + str(val_n_pts_total)]\n",
        "#                 + [\"train_error_CO_\" + str(i+1) for i in range(len(train_errors[0]))]\n",
        "#                 + [\"val_error_CO_\" + str(i+1) for i in range(len(val_errors[0]))])\n",
        "# run_data = np.dstack(tuple(columns_data))[0]\n",
        "# RunData_df = pd.DataFrame(data=run_data, columns=columns_names)\n",
        "# SaveDataset(RunData_df, run_save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:31.758319Z",
          "iopub.execute_input": "2021-11-12T17:35:31.759034Z",
          "iopub.status.idle": "2021-11-12T17:35:31.771215Z",
          "shell.execute_reply.started": "2021-11-12T17:35:31.758993Z",
          "shell.execute_reply": "2021-11-12T17:35:31.770551Z"
        },
        "trusted": true,
        "id": "ympS-eXqTn2X"
      },
      "source": [
        "# ReadDataset(run_save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixTYCugTTn2X"
      },
      "source": [
        "## Final Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCrjU1TjTn2X"
      },
      "source": [
        "colsample_bytree and alpha ARE NOT USED by AdaBoost - was used in other algos\n",
        "\n",
        "Given here so that same code can be used for training other algorithms\n",
        "\n",
        "### Final Tuned HyperParams\n",
        "\n",
        " - CO_1 - AdaBoost with Decision Tree base_estimator, 0.2 Learning Rate, 1 Max Depth, 200 Estimators, Seed 0\n",
        " - CO_2 - AdaBoost with Decision Tree base_estimator, 0.2 Learning Rate, 1 Max Depth, 200 Estimators, Seed 0\n",
        " - CO_3 - AdaBoost with Decision Tree base_estimator, 0.2 Learning Rate, 1 Max Depth, 200 Estimators, Seed 0\n",
        " - CO_4 - AdaBoost with Decision Tree base_estimator, 0.2 Learning Rate, 1 Max Depth, 200 Estimators, Seed 0\n",
        " - CO_5 - AdaBoost with Decision Tree base_estimator, 0.25 Learning Rate, 1 Max Depth, 30 Estimators, Seed 0\n",
        " - CO_6 - AdaBoost with Decision Tree base_estimator, 0.25 Learning Rate, 1 Max Depth, 30 Estimators, Seed 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:35:31.772358Z",
          "iopub.execute_input": "2021-11-12T17:35:31.773048Z",
          "iopub.status.idle": "2021-11-12T17:54:21.558532Z",
          "shell.execute_reply.started": "2021-11-12T17:35:31.773009Z",
          "shell.execute_reply": "2021-11-12T17:54:21.557555Z"
        },
        "trusted": true,
        "id": "dIujWQwVTn2X"
      },
      "source": [
        "# Choose Best Classifier HyperParams from Tuning\n",
        "# print(np.array(train_errors).shape, np.array(val_errors).shape)\n",
        "# total_errors = np.sum(train_errors, axis=1) + np.sum(val_errors, axis=1)\n",
        "# best_classifier_index = np.argmin(total_errors)\n",
        "# Train on Whole Train Dataset with best HyperParams\n",
        "# best_params = HyperParamsTests[best_classifier_index]\n",
        "\n",
        "# Use Original Dataset for CO_1 to CO_4, Scaled Dataset for CO_5 and CO_6\n",
        "# No Scaling for first 4\n",
        "for i in range(4):\n",
        "    train_data_pts[i] = np.copy(train_data_pts_features[i])\n",
        "    test_data_pts[i] = np.copy(test_data_pts_features[i])\n",
        "\n",
        "# Set HyperParams\n",
        "best_params = [\n",
        "        [AdaBoost_Run, 0.0, 0.2, 1, 10, 200, 0],\n",
        "        [AdaBoost_Run, 0.0, 0.2, 1, 10, 200, 0],\n",
        "        [AdaBoost_Run, 0.0, 0.2, 1, 10, 200, 0],\n",
        "        [AdaBoost_Run, 0.0, 0.2, 1, 10, 200, 0],\n",
        "        [AdaBoost_Run, 0.0, 0.25, 1, 10, 30, 0],\n",
        "        [AdaBoost_Run, 0.0, 0.25, 1, 10, 30, 0],\n",
        "    ]\n",
        "best_params = list(np.array(best_params).T)\n",
        "print(\"RUNNING FINAL TRAIN:\", i+1)\n",
        "print(\"PARAMS: colsample_bytree, learning_rate, max_depth, alpha, n_estimators, seed\")\n",
        "print(\"VALUES:\", best_params)\n",
        "trained_classifiers, train_missclassified_counts, curCOs = RunContestTraining(\n",
        "        train_data_pts, train_targets, \n",
        "       best_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cZm7DKoTn2Y"
      },
      "source": [
        "# Save Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:54:21.560413Z",
          "iopub.execute_input": "2021-11-12T17:54:21.560682Z",
          "iopub.status.idle": "2021-11-12T17:54:37.028694Z",
          "shell.execute_reply.started": "2021-11-12T17:54:21.560649Z",
          "shell.execute_reply": "2021-11-12T17:54:37.027742Z"
        },
        "trusted": true,
        "id": "mny7MgubTn2Y"
      },
      "source": [
        "# Predict on Test Dataset\n",
        "Predictions = []\n",
        "for i in range(len(trained_classifiers)):\n",
        "    trained_classifier = trained_classifiers[i]\n",
        "    y_pred = trained_classifier.predict(test_data_pts[i])\n",
        "    Predictions.append([int(round(yp, 0)) for yp in list(y_pred)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-12T17:54:37.030306Z",
          "iopub.execute_input": "2021-11-12T17:54:37.030669Z",
          "iopub.status.idle": "2021-11-12T17:54:37.042482Z",
          "shell.execute_reply.started": "2021-11-12T17:54:37.030622Z",
          "shell.execute_reply": "2021-11-12T17:54:37.041507Z"
        },
        "trusted": true,
        "id": "K7RYWD11Tn2Y"
      },
      "source": [
        "# Save Predictions\n",
        "preds_save_path = \"./submission.csv\"\n",
        "SaveContestPredictions(Predictions, preds_save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gLPSHpoTn2Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}