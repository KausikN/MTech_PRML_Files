{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# Imports\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:05.780190Z","iopub.execute_input":"2021-11-12T17:35:05.780932Z","iopub.status.idle":"2021-11-12T17:35:05.785426Z","shell.execute_reply.started":"2021-11-12T17:35:05.780891Z","shell.execute_reply":"2021-11-12T17:35:05.784828Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Functions","metadata":{}},{"cell_type":"code","source":"# Imports\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Main Functions\n# Dataset Functions\ndef ReadDataset(path):\n    dataset = pd.read_csv(path)\n    return dataset\n\ndef SaveDataset(dataset, path):\n    dataset.to_csv(path, index=False)\n\ndef FormatDataset(dataset, targetRowCount=0):\n    data_pts = []\n    targets = []\n    if targetRowCount == 0:\n        data_pts = dataset.iloc[:, 1:].values.T\n        targets = np.array([])\n    else:\n        data_pts = dataset.iloc[:-targetRowCount, 1:].values.T\n        targets = dataset.iloc[-targetRowCount:, 1:].values\n\n    print(\"Data points: \", data_pts.shape)\n    print(\"Targets: \", targets.shape)\n\n    return data_pts, targets\n\n# Specific Functions\ndef LoadContestDatasets(train_dataset_path_1, test_dataset_path_1, train_dataset_path_2, test_dataset_path_2):\n    # Dataset 1\n    print(\"Train Dataset 1\")\n    train_dataset_1 = ReadDataset(train_dataset_path_1)\n    train_data_pts_1, train_targets_1 = FormatDataset(train_dataset_1, targetRowCount=2)\n    print(\"Test Dataset 1\")\n    test_dataset_1 = ReadDataset(test_dataset_path_1)\n    test_data_pts_1, _ = FormatDataset(test_dataset_1, targetRowCount=0)\n\n    # Dataset 2\n    print(\"Train Dataset 2\")\n    train_dataset_2 = ReadDataset(train_dataset_path_2)\n    train_data_pts_2, train_targets_2 = FormatDataset(train_dataset_2, targetRowCount=4)\n    print(\"Test Dataset 2\")\n    test_dataset_2 = ReadDataset(test_dataset_path_2)\n    test_data_pts_2, _ = FormatDataset(test_dataset_2, targetRowCount=0)\n\n    return train_data_pts_1, train_targets_1, test_data_pts_1, train_data_pts_2, train_targets_2, test_data_pts_2\n\ndef SaveContestPredictions(Predictions, preds_save_path):\n    # Append all predictions to single list\n    Predictions_Appended = []\n    for preds in Predictions:\n        Predictions_Appended.extend(preds)\n    Predictions_Appended = [int(round(pred, 0)) for pred in Predictions_Appended]\n    print(\"Predictions Count:\", len(Predictions_Appended))\n\n    # Form a dataframe\n    indices = list(range(len(Predictions_Appended)))\n    preds_data = np.dstack((indices, Predictions_Appended))[0]\n    Predictions_df = pd.DataFrame(data=preds_data, columns=[\"Id\", \"Predicted\"])\n    SaveDataset(Predictions_df, preds_save_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:06.123789Z","iopub.execute_input":"2021-11-12T17:35:06.124315Z","iopub.status.idle":"2021-11-12T17:35:06.140003Z","shell.execute_reply.started":"2021-11-12T17:35:06.124272Z","shell.execute_reply":"2021-11-12T17:35:06.139150Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# Dataset Paths\ntrain_dataset_path_1 = \"../input/dataset-prml-data-contest-1/Dataset_1_Training.csv\"\ntest_dataset_path_1 = \"../input/dataset-prml-data-contest-1/Dataset_1_Testing.csv\"\ntrain_dataset_path_2 = \"../input/dataset-prml-data-contest-1/Dataset_2_Training.csv\"\ntest_dataset_path_2 = \"../input/dataset-prml-data-contest-1/Dataset_2_Testing.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:06.297691Z","iopub.execute_input":"2021-11-12T17:35:06.298629Z","iopub.status.idle":"2021-11-12T17:35:06.302535Z","shell.execute_reply.started":"2021-11-12T17:35:06.298579Z","shell.execute_reply":"2021-11-12T17:35:06.301931Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Load Datasets\ntrain_data_pts_1_original, train_targets_1, test_data_pts_1_original, train_data_pts_2_original, train_targets_2, test_data_pts_2_original = \\\n    LoadContestDatasets(train_dataset_path_1, test_dataset_path_1, train_dataset_path_2, test_dataset_path_2)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:06.461923Z","iopub.execute_input":"2021-11-12T17:35:06.462809Z","iopub.status.idle":"2021-11-12T17:35:12.848004Z","shell.execute_reply.started":"2021-11-12T17:35:06.462764Z","shell.execute_reply":"2021-11-12T17:35:12.847030Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Visualisations","metadata":{}},{"cell_type":"code","source":"# Visualisation Functions\ndef PlotDatasetCovariance(data_pts, title=\"\"):\n    # Visualise the covariance\n    cov_matrix = np.cov(data_pts)\n    plt.imshow(cov_matrix, cmap='viridis')\n    plt.colorbar()\n    plt.title(title)\n    plt.show()\n\ndef PlotDatasetMean(data_pts, title=\"\"):\n    # Visualise the mean\n    mean_data_pt = np.mean(data_pts, axis=0)\n    size = mean_data_pt.shape[0]\n\n    # Plot\n    plt.plot(list(range(size)), mean_data_pt)\n    plt.title(title)\n    plt.show()\n\ndef PlotDatasetVariance(data_pts, title=\"\"):\n    # Visualise the variance\n    variance = np.var(data_pts, axis=0)\n    size = variance.shape[0]\n\n    # Plot\n    plt.plot(list(range(size)), variance)\n    plt.title(title)\n    plt.show()\n    \ndef ShowClassImbalanceDataset(targets, title=''):\n    # Calculate the class imbalance\n    targets = np.array(targets, dtype=int)\n    class_counts = np.bincount(targets)\n    # class_counts = class_counts[1:]\n    # class_counts = class_counts / np.sum(class_counts)\n\n    # Show the class imbalance\n    plt.bar(list(range(len(class_counts))), class_counts)\n    plt.title(title)\n    plt.show()\n    \ndef PlotDatasetMeanBins(data_pts, bins=100, title=\"\"):\n    # Visualise the mean\n    mean = np.mean(data_pts, axis=0)\n    size = mean.shape[0]\n\n    # Plot\n    plt.hist(mean, bins=bins)\n    plt.title(title)\n    plt.show()\n\ndef PlotDatasetVarianceBins(data_pts, bins=100, title=\"\"):\n    # Visualise the variance\n    variance = np.var(data_pts, axis=0)\n    size = variance.shape[0]\n    \n    # Plot\n    plt.hist(variance, bins=bins)\n    plt.title(title)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:12.850339Z","iopub.execute_input":"2021-11-12T17:35:12.850555Z","iopub.status.idle":"2021-11-12T17:35:12.865373Z","shell.execute_reply.started":"2021-11-12T17:35:12.850529Z","shell.execute_reply":"2021-11-12T17:35:12.864382Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"print(\"TRAIN DATA VISUALISATION - ORIGINAL\")\n# Train Dataset 1\n# Visualise Covariance\nPlotDatasetCovariance(train_data_pts_1_original, \"Original Train Dataset 1 Covariance\")\n\n# Visualise Mean\nPlotDatasetMean(train_data_pts_1_original, \"Original Train Dataset 1 Mean\")\nPlotDatasetMeanBins(train_data_pts_1_original, 100, \"Original Train Dataset 1 Mean Histogram\")\n\n# Visualise Variance\nPlotDatasetVariance(train_data_pts_1_original, \"Original Train Dataset 1 Variance\")\nPlotDatasetVarianceBins(train_data_pts_1_original, 100, \"Original Train Dataset 1 Variance Histogram\")\n\n# Train Dataset 2\n# Visualise Covariance\nPlotDatasetCovariance(train_data_pts_2_original, \"Original Train Dataset 2 Covariance\")\n\n# Visualise Mean\nPlotDatasetMean(train_data_pts_2_original, \"Original Train Dataset 2 Mean\")\nPlotDatasetMeanBins(train_data_pts_2_original, 100, \"Original Train Dataset 2 Mean Histogram\")\n\n# Visualise Variance\nPlotDatasetVariance(train_data_pts_2_original, \"Original Train Dataset 2 Variance\")\nPlotDatasetVarianceBins(train_data_pts_2_original, 100, \"Original Train Dataset 2 Variance Histogram\")","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:12.866721Z","iopub.execute_input":"2021-11-12T17:35:12.867022Z","iopub.status.idle":"2021-11-12T17:35:16.279215Z","shell.execute_reply.started":"2021-11-12T17:35:12.866990Z","shell.execute_reply":"2021-11-12T17:35:16.278133Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"print(\"TEST DATA VISUALISATION - ORIGINAL\")\n# Test Dataset 1\n# Visualise Covariance\nPlotDatasetCovariance(test_data_pts_1_original, \"Original Test Dataset 1 Covariance\")\n\n# Visualise Mean\nPlotDatasetMean(test_data_pts_1_original, \"Original Test Dataset 1 Mean\")\nPlotDatasetMeanBins(test_data_pts_1_original, 100, \"Original Test Dataset 1 Mean Histogram\")\n\n# Visualise Variance\nPlotDatasetVariance(test_data_pts_1_original, \"Original Test Dataset 1 Variance\")\nPlotDatasetVarianceBins(test_data_pts_1_original, 100, \"Original Test Dataset 1 Variance Histogram\")\n\n# Test Dataset 2\n# Visualise Covariance\nPlotDatasetCovariance(test_data_pts_2_original, \"Original Test Dataset 2 Covariance\")\n\n# Visualise Mean\nPlotDatasetMean(test_data_pts_2_original, \"Original Test Dataset 2 Mean\")\nPlotDatasetMeanBins(test_data_pts_2_original, 100, \"Original Test Dataset 2 Mean Histogram\")\n\n# Visualise Variance\nPlotDatasetVariance(test_data_pts_2_original, \"Original Test Dataset 2 Variance\")\nPlotDatasetVarianceBins(test_data_pts_2_original, 100, \"Original Test Dataset 2 Variance Histogram\")","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:16.282012Z","iopub.execute_input":"2021-11-12T17:35:16.282440Z","iopub.status.idle":"2021-11-12T17:35:19.870881Z","shell.execute_reply.started":"2021-11-12T17:35:16.282392Z","shell.execute_reply":"2021-11-12T17:35:19.870124Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Preprocessing","metadata":{}},{"cell_type":"code","source":"train_data_pts_features = []\ntest_data_pts_features = []\n\ntrain_targets = list(train_targets_1)\ntrain_targets.extend(train_targets_2)\n\nfor i in range(2):\n    train_data_pts_features.append(np.copy(train_data_pts_1_original))\n    test_data_pts_features.append(np.copy(test_data_pts_1_original))\nfor i in range(4):\n    train_data_pts_features.append(np.copy(train_data_pts_2_original))\n    test_data_pts_features.append(np.copy(test_data_pts_2_original))\n\n# Print Dataset Shapes\nprint(\"TRAIN DATASETS: \")\nfor i in range(len(train_data_pts_features)):\n    print(\"CO_\" + str(i+1) + \" Data points: \", train_data_pts_features[i].shape)\n\nprint(\"TEST DATASETS: \")\nfor i in range(len(test_data_pts_features)):\n    print(\"CO_\" + str(i+1) + \" Data points: \", test_data_pts_features[i].shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:19.872139Z","iopub.execute_input":"2021-11-12T17:35:19.872363Z","iopub.status.idle":"2021-11-12T17:35:20.309997Z","shell.execute_reply.started":"2021-11-12T17:35:19.872335Z","shell.execute_reply":"2021-11-12T17:35:20.309081Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# Imports\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# Preprocessing Functions\ndef MinMaxScaleDataset(dataset_pts):\n    scaler = MinMaxScaler()\n    scaled = scaler.fit_transform(dataset_pts)\n    return scaled\n\ndef StandardScaleDataset(dataset_pts):\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(dataset_pts)\n    return scaled\n\ndef CombinedStandardScaleDataset(datasets_pts):\n    scaler = StandardScaler()\n    scaled_train = scaler.fit_transform(datasets_pts[0])\n    scaled_test = scaler.transform(datasets_pts[1])\n    return scaled_train, scaled_test","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:20.311866Z","iopub.execute_input":"2021-11-12T17:35:20.312200Z","iopub.status.idle":"2021-11-12T17:35:20.319633Z","shell.execute_reply.started":"2021-11-12T17:35:20.312156Z","shell.execute_reply":"2021-11-12T17:35:20.318559Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"train_data_pts = []\ntest_data_pts = []\n\n# # Original Datasets\n# for pts in train_data_pts_features:\n#     train_data_pts.append(np.copy(pts))\n# for pts in test_data_pts_features:\n#     test_data_pts.append(np.copy(pts))\n\n# # MinMaxScale Datasets Separately\n# for pts in train_data_pts_features:\n#     pts_scaled = MinMaxScaleDataset(pts)\n#     train_data_pts.append(pts_scaled)\n# for pts in test_data_pts_features:\n#     pts_scaled = MinMaxScaleDataset(pts)\n#     test_data_pts.append(pts_scaled)\n    \n# StandardScale Datasets Separately\nfor pts in train_data_pts_features:\n    pts_scaled = StandardScaleDataset(pts)\n    train_data_pts.append(pts_scaled)\nfor pts in test_data_pts_features:\n    pts_scaled = StandardScaleDataset(pts)\n    test_data_pts.append(pts_scaled)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:20.321764Z","iopub.execute_input":"2021-11-12T17:35:20.322111Z","iopub.status.idle":"2021-11-12T17:35:23.266334Z","shell.execute_reply.started":"2021-11-12T17:35:20.322066Z","shell.execute_reply":"2021-11-12T17:35:23.265359Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessed Dataset Visualisations","metadata":{}},{"cell_type":"code","source":"print(\"TRAIN DATA CLASSES\")\n# Visualise Class Imbalance\nco = 0\nfor i in range(train_targets_1.shape[0]):\n    co += 1\n    ShowClassImbalanceDataset(train_targets_1[i], \"Train Dataset 1 CO_\" + str(co) + \" Classes\")\nfor i in range(train_targets_2.shape[0]):\n    co += 1\n    ShowClassImbalanceDataset(train_targets_2[i], \"Train Dataset 2 CO_\" + str(co) + \" Classes\")\n\nprint(\"TRAIN DATA VISUALISATION - SCALED\")\n# Train Dataset 1\n# Visualise Covariance\nPlotDatasetCovariance(train_data_pts[0], \"Train Dataset 1 Covariance\")\n\n# Visualise Mean\nPlotDatasetMean(train_data_pts[0], \"Train Dataset 1 Mean\")\nPlotDatasetMeanBins(train_data_pts[0], 100, \"Train Dataset 1 Mean Histogram\")\n\n# Visualise Variance\nPlotDatasetVariance(train_data_pts[0], \"Train Dataset 1 Variance\")\nPlotDatasetVarianceBins(train_data_pts[0], 100, \"Train Dataset 1 Variance Histogram\")\n\n# Train Dataset 2\n# Visualise Covariance\nPlotDatasetCovariance(train_data_pts[-1], \"Train Dataset 2 Covariance\")\n\n# Visualise Mean\nPlotDatasetMean(train_data_pts[-1], \"Train Dataset 2 Mean\")\nPlotDatasetMeanBins(train_data_pts[-1], 100, \"Train Dataset 2 Mean Histogram\")\n\n# Visualise Variance\nPlotDatasetVariance(train_data_pts[-1], \"Train Dataset 2 Variance\")\nPlotDatasetVarianceBins(train_data_pts[-1], 100, \"Train Dataset 2 Variance Histogram\")","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:23.267742Z","iopub.execute_input":"2021-11-12T17:35:23.267990Z","iopub.status.idle":"2021-11-12T17:35:28.321500Z","shell.execute_reply.started":"2021-11-12T17:35:23.267959Z","shell.execute_reply":"2021-11-12T17:35:28.320609Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"print(\"TEST DATA VISUALISATION - SCALED\")\n# Test Dataset 1\n# Visualise Covariance\nPlotDatasetCovariance(test_data_pts[0], \"Test Dataset 1 Covariance\")\n\n# Visualise Mean\nPlotDatasetMean(test_data_pts[0], \"Test Dataset 1 Mean\")\nPlotDatasetMeanBins(test_data_pts[0], 100, \"Test Dataset 1 Mean Histogram\")\n\n# Visualise Variance\nPlotDatasetVariance(test_data_pts[0], \"Test Dataset 1 Variance\")\nPlotDatasetVarianceBins(test_data_pts[0], 100, \"Test Dataset 1 Variance Histogram\")\n\n# Test Dataset 2\n# Visualise Covariance\nPlotDatasetCovariance(test_data_pts[-1], \"Test Dataset 2 Covariance\")\n\n# Visualise Mean\nPlotDatasetMean(test_data_pts[-1], \"Test Dataset 2 Mean\")\nPlotDatasetMeanBins(test_data_pts[-1], 100, \"Test Dataset 2 Mean Histogram\")\n\n# Visualise Variance\nPlotDatasetVariance(test_data_pts[-1], \"Test Dataset 2 Variance\")\nPlotDatasetVarianceBins(test_data_pts[-1], 100, \"Test Dataset 2 Variance Histogram\")","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:28.322781Z","iopub.execute_input":"2021-11-12T17:35:28.323027Z","iopub.status.idle":"2021-11-12T17:35:31.659637Z","shell.execute_reply.started":"2021-11-12T17:35:28.323001Z","shell.execute_reply":"2021-11-12T17:35:31.658871Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"# Classifiers","metadata":{}},{"cell_type":"markdown","source":"## AdaBoost","metadata":{}},{"cell_type":"code","source":"# Imports\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Main Functions\n# AdaBoost Functions\ndef AdaBoost_Run(train_data_pts, train_targets, colsample_bytree=0.3, learning_rate=0.1, max_depth=10, alpha=10, n_estimators=200, seed=0):\n    X_train, X_test, y_train, y_test = train_data_pts, train_data_pts, train_targets, train_targets\n\n    classifier = AdaBoostClassifier(\n#         RandomForestClassifier( n_estimators=5,\n        DecisionTreeClassifier(\n            max_depth=int(max_depth), random_state=int(seed)\n        ), \n        n_estimators=int(n_estimators), learning_rate=learning_rate, random_state=int(seed))\n    trained_classifier = classifier.fit(X_train, y_train)\n\n    # Print Results\n    y_pred = trained_classifier.predict(X_test)\n    missclassified_count = (y_test != y_pred).sum()\n    print(\"Missclassified:\", missclassified_count, \"/\", X_test.shape[0])\n    print(\"\\n\")\n        \n    return trained_classifier, missclassified_count","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:31.662081Z","iopub.execute_input":"2021-11-12T17:35:31.662315Z","iopub.status.idle":"2021-11-12T17:35:31.670631Z","shell.execute_reply.started":"2021-11-12T17:35:31.662289Z","shell.execute_reply":"2021-11-12T17:35:31.669826Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def RunContestTraining(Xs, Ys, \n                       params):\n    ALGO = params[0]\n    colsample_bytree=params[1]\n    learning_rate=params[2]\n    max_depth=params[3]\n    alpha=params[4]\n    n_estimators=params[5]\n    seed=params[6]\n    trained_classifiers = []\n    missclassified_counts = []\n    curCOs = []\n\n    #### DATASET 1 ####\n    # Train Classifier\n    print(\"DATASET 1\")\n\n    # CO_1\n    curCO = 1\n    print(\"For C\" + str(curCO) + \": \")\n    trained_classifier, missclassified_count = ALGO[0](Xs[0], Ys[0], # MODEL CHANGE\n        colsample_bytree=colsample_bytree[0], learning_rate=learning_rate[0], max_depth=max_depth[0], \n                                      alpha=alpha[0], n_estimators=n_estimators[0], seed=seed[0])\n    trained_classifiers.append(trained_classifier)\n    missclassified_counts.append(missclassified_count)\n    curCOs.append(curCO)\n    print()\n\n    # CO_2\n    curCO = 2\n    print(\"For C\" + str(curCO) + \": \")\n    trained_classifier, missclassified_count = ALGO[1](Xs[1], Ys[1], # MODEL CHANGE\n        colsample_bytree=colsample_bytree[1], learning_rate=learning_rate[1], max_depth=max_depth[1], \n                                      alpha=alpha[1], n_estimators=n_estimators[1], seed=seed[1])\n    trained_classifiers.append(trained_classifier)\n    missclassified_counts.append(missclassified_count)\n    curCOs.append(curCO)\n    print()\n\n\n    #### DATASET 2 ####\n    # Train Classifier\n    print(\"DATASET 2\")\n\n    # CO_3\n    curCO = 3\n    print(\"For C\" + str(curCO) + \": \")\n    trained_classifier, missclassified_count = ALGO[2](Xs[2], Ys[2], # MODEL CHANGE\n        colsample_bytree=colsample_bytree[2], learning_rate=learning_rate[2], max_depth=max_depth[2], \n                                      alpha=alpha[2], n_estimators=n_estimators[2], seed=seed[2])\n    trained_classifiers.append(trained_classifier)\n    missclassified_counts.append(missclassified_count)\n    curCOs.append(curCO)\n    print()\n\n    # CO_4\n    curCO = 4\n    print(\"For C\" + str(curCO) + \": \")\n    trained_classifier, missclassified_count = ALGO[3](Xs[3], Ys[3], # MODEL CHANGE\n        colsample_bytree=colsample_bytree[3], learning_rate=learning_rate[3], max_depth=max_depth[3], \n                                      alpha=alpha[3], n_estimators=n_estimators[3], seed=seed[3])\n    trained_classifiers.append(trained_classifier)\n    missclassified_counts.append(missclassified_count)\n    curCOs.append(curCO)\n    print()\n\n    # CO_5\n    curCO = 5\n    print(\"For C\" + str(curCO) + \": \")\n    trained_classifier, missclassified_count = ALGO[4](Xs[4], Ys[4], # MODEL CHANGE\n        colsample_bytree=colsample_bytree[4], learning_rate=learning_rate[4], max_depth=max_depth[4], \n                                      alpha=alpha[4], n_estimators=n_estimators[4], seed=seed[4])\n    trained_classifiers.append(trained_classifier)\n    missclassified_counts.append(missclassified_count)\n    curCOs.append(curCO)\n    print()\n\n    # CO_6\n    curCO = 6\n    print(\"For C\" + str(curCO) + \": \")\n    trained_classifier, missclassified_count = ALGO[5](Xs[5], Ys[5], # MODEL CHANGE\n        colsample_bytree=colsample_bytree[5], learning_rate=learning_rate[5], max_depth=max_depth[5], \n                                      alpha=alpha[5], n_estimators=n_estimators[5], seed=seed[5])\n    trained_classifiers.append(trained_classifier)\n    missclassified_counts.append(missclassified_count)\n    curCOs.append(curCO)\n    print()\n    \n    return trained_classifiers, missclassified_counts, curCOs","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:31.672163Z","iopub.execute_input":"2021-11-12T17:35:31.672374Z","iopub.status.idle":"2021-11-12T17:35:31.693313Z","shell.execute_reply.started":"2021-11-12T17:35:31.672348Z","shell.execute_reply":"2021-11-12T17:35:31.692596Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def RunContestPredict(Xs, Ys, trained_classifiers, curCOs):\n    # Predict on Validation Dataset\n    missclassified_counts = []\n    for i in range(len(trained_classifiers)):\n        trained_classifier = trained_classifiers[i]\n        y_pred = trained_classifier.predict(Xs[i])\n        missclassified_count = (Ys[i] != y_pred).sum()\n        print(\"For C\" + str(curCOs[i]) + \": \")\n        print(\"Missclassified:\", missclassified_count, \"/\", Xs[i].shape[0])\n        print(\"\\n\")\n        missclassified_counts.append(missclassified_count)\n    \n    return missclassified_counts","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:31.694438Z","iopub.execute_input":"2021-11-12T17:35:31.694706Z","iopub.status.idle":"2021-11-12T17:35:31.704913Z","shell.execute_reply.started":"2021-11-12T17:35:31.694675Z","shell.execute_reply":"2021-11-12T17:35:31.704106Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"## HyperParameter Tuning and Validation","metadata":{}},{"cell_type":"code","source":"# HyperParamsTests = [\n#     [\n#         [AdaBoost_Run, 0.75, 0.2, 1, 10, 200, 0],\n#         [AdaBoost_Run, 0.75, 0.2, 1, 10, 200, 0],\n#         [AdaBoost_Run, 0.75, 0.2, 1, 10, 200, 0],\n#         [AdaBoost_Run, 0.75, 0.2, 1, 10, 200, 0],\n#         [AdaBoost_Run, 0.75, 0.25, 1, 10, 30, 0],\n#         [AdaBoost_Run, 0.75, 0.25, 1, 10, 30, 0],\n#     ]\n# ]","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:31.705929Z","iopub.execute_input":"2021-11-12T17:35:31.706456Z","iopub.status.idle":"2021-11-12T17:35:31.715515Z","shell.execute_reply.started":"2021-11-12T17:35:31.706419Z","shell.execute_reply":"2021-11-12T17:35:31.714986Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# # Dataset Split for Validation\n# X_trains = []\n# Y_trains = []\n# X_vals = []\n# Y_vals = []\n\n# for i in range(len(train_data_pts)):\n#     X_train, X_test, y_train, y_test = train_test_split(train_data_pts[i], train_targets[i].T, \n#                 test_size=0.2, random_state=0) # SPLIT SEED -- CHANGE\n#     X_trains.append(X_train)\n#     Y_trains.append(y_train.T)\n#     X_vals.append(X_test)\n#     Y_vals.append(y_test.T)\n\n#     print(\"DATASET CO_\" + str(i+1))\n#     print(\"Train: X:\", X_train.shape, \"Y:\", y_train.shape)\n#     print(\"Val: X:\", X_test.shape, \"Y:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:31.716795Z","iopub.execute_input":"2021-11-12T17:35:31.717241Z","iopub.status.idle":"2021-11-12T17:35:31.732796Z","shell.execute_reply.started":"2021-11-12T17:35:31.717209Z","shell.execute_reply":"2021-11-12T17:35:31.731938Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# # Run Contest Training and Prediction on Grid of Hyperparams\n# train_errors = []\n# val_errors = []\n# runs_trained_classifiers = []\n# for i in tqdm(range(len(HyperParamsTests))):\n#     params = list(np.array(HyperParamsTests[i]).T)\n#     print(\"RUNNING TEST:\", i+1)\n#     print(\"PARAMS: colsample_bytree, learning_rate, max_depth, alpha, n_estimators, seed\")\n#     print(\"VALUES:\", params)\n#     print()\n#     print(\"Training...\")\n#     trained_classifiers, train_missclassified_counts, curCOs = RunContestTraining(X_trains, Y_trains, \n#                            params)\n#     print(\"Validating...\")\n#     val_missclassified_counts = RunContestPredict(X_vals, Y_vals, trained_classifiers, curCOs)\n# #     train_missclassified_counts = [0, 0, 0, 0, 0, 0]\n# #     val_missclassified_counts = [0, 0, 0, 0, 0, 0]\n# #     trained_classifiers = [None, None, None, None, None]\n#     train_errors.append(train_missclassified_counts)\n#     val_errors.append(val_missclassified_counts)\n#     runs_trained_classifiers.append(trained_classifiers)\n#     print()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:31.734509Z","iopub.execute_input":"2021-11-12T17:35:31.735193Z","iopub.status.idle":"2021-11-12T17:35:31.744295Z","shell.execute_reply.started":"2021-11-12T17:35:31.735151Z","shell.execute_reply":"2021-11-12T17:35:31.743741Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# # Save Test Run Errors and Data\n# run_save_path = \"./run_data.csv\"\n\n# HyperParams = []\n# for hparams in HyperParamsTests:\n#     hparams = np.array(hparams).T\n#     hparams_joined = [\", \".join([f.__name__ for f in hparams[0]])]\n#     for p in hparams[1:]:\n#         hparams_joined.append(\", \".join(list(map(str, p))))\n#     HyperParams.append(hparams_joined)\n# HyperParams = np.array(HyperParams).T\n# HyperParamsColumnNames = [\"algo\", \"colsample_bytree\", \"learning_rate\", \"max_depth\", \"alpha\", \"n_estimators\", \"seed\"]\n\n# # Form a dataframe\n# train_n_pts = []\n# val_n_pts = []\n# for i in range(len(X_trains)):\n#     train_n_pts.append(X_trains[i].shape[0])\n#     val_n_pts.append(X_vals[i].shape[0])\n# train_n_pts_total = sum(train_n_pts)\n# val_n_pts_total = sum(val_n_pts)\n# train_n_pts_array = np.array(train_n_pts)\n# val_n_pts_array = np.array(val_n_pts)\n\n# total_train_errors_stacked = list((np.sum(train_errors, axis=1) / (train_n_pts_total)).T)\n# total_val_errors_stacked = list((np.sum(val_errors, axis=1) / (val_n_pts_total)).T)\n# train_errors_stacked = list((np.array(train_errors) / (train_n_pts_array)).T)\n# val_errors_stacked = list((np.array(val_errors) / (val_n_pts_array)).T)\n# indices = list(range(len(train_errors)))\n# columns_data = ([indices]\n#                 + list(HyperParams)\n#                 + [total_train_errors_stacked, total_val_errors_stacked]\n#                 + train_errors_stacked + val_errors_stacked)\n# # columns_data = np.array(columns_data)\n# columns_names = ([\"id\"] + HyperParamsColumnNames\n#                 + [\"total_train_error_\" + str(train_n_pts_total), \"total_val_error_\" + str(val_n_pts_total)]\n#                 + [\"train_error_CO_\" + str(i+1) for i in range(len(train_errors[0]))]\n#                 + [\"val_error_CO_\" + str(i+1) for i in range(len(val_errors[0]))])\n# run_data = np.dstack(tuple(columns_data))[0]\n# RunData_df = pd.DataFrame(data=run_data, columns=columns_names)\n# SaveDataset(RunData_df, run_save_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:31.745526Z","iopub.execute_input":"2021-11-12T17:35:31.745932Z","iopub.status.idle":"2021-11-12T17:35:31.757208Z","shell.execute_reply.started":"2021-11-12T17:35:31.745902Z","shell.execute_reply":"2021-11-12T17:35:31.756639Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# ReadDataset(run_save_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:31.758319Z","iopub.execute_input":"2021-11-12T17:35:31.759034Z","iopub.status.idle":"2021-11-12T17:35:31.771215Z","shell.execute_reply.started":"2021-11-12T17:35:31.758993Z","shell.execute_reply":"2021-11-12T17:35:31.770551Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"## Final Training","metadata":{}},{"cell_type":"markdown","source":"colsample_bytree and alpha ARE NOT USED by AdaBoost - was used in other algos\n\nGiven here so that same code can be used for training other algorithms\n\n### Final Tuned HyperParams\n\n - CO_1 - AdaBoost with Decision Tree base_estimator, 0.2 Learning Rate, 1 Max Depth, 200 Estimators, Seed 0\n - CO_2 - AdaBoost with Decision Tree base_estimator, 0.2 Learning Rate, 1 Max Depth, 200 Estimators, Seed 0\n - CO_3 - AdaBoost with Decision Tree base_estimator, 0.2 Learning Rate, 1 Max Depth, 200 Estimators, Seed 0\n - CO_4 - AdaBoost with Decision Tree base_estimator, 0.2 Learning Rate, 1 Max Depth, 200 Estimators, Seed 0\n - CO_5 - AdaBoost with Decision Tree base_estimator, 0.25 Learning Rate, 1 Max Depth, 30 Estimators, Seed 0\n - CO_6 - AdaBoost with Decision Tree base_estimator, 0.25 Learning Rate, 1 Max Depth, 30 Estimators, Seed 0","metadata":{}},{"cell_type":"code","source":"# Choose Best Classifier HyperParams from Tuning\n# print(np.array(train_errors).shape, np.array(val_errors).shape)\n# total_errors = np.sum(train_errors, axis=1) + np.sum(val_errors, axis=1)\n# best_classifier_index = np.argmin(total_errors)\n# Train on Whole Train Dataset with best HyperParams\n# best_params = HyperParamsTests[best_classifier_index]\n\n# Use Original Dataset for CO_1 to CO_4, Scaled Dataset for CO_5 and CO_6\n# No Scaling for first 4\nfor i in range(4):\n    train_data_pts[i] = np.copy(train_data_pts_features[i])\n    test_data_pts[i] = np.copy(test_data_pts_features[i])\n\n# Set HyperParams\nbest_params = [\n        [AdaBoost_Run, 0.0, 0.2, 1, 10, 200, 0],\n        [AdaBoost_Run, 0.0, 0.2, 1, 10, 200, 0],\n        [AdaBoost_Run, 0.0, 0.2, 1, 10, 200, 0],\n        [AdaBoost_Run, 0.0, 0.2, 1, 10, 200, 0],\n        [AdaBoost_Run, 0.0, 0.25, 1, 10, 30, 0],\n        [AdaBoost_Run, 0.0, 0.25, 1, 10, 30, 0],\n    ]\nbest_params = list(np.array(best_params).T)\nprint(\"RUNNING FINAL TRAIN:\", i+1)\nprint(\"PARAMS: colsample_bytree, learning_rate, max_depth, alpha, n_estimators, seed\")\nprint(\"VALUES:\", best_params)\ntrained_classifiers, train_missclassified_counts, curCOs = RunContestTraining(\n        train_data_pts, train_targets, \n       best_params)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:35:31.772358Z","iopub.execute_input":"2021-11-12T17:35:31.773048Z","iopub.status.idle":"2021-11-12T17:54:21.558532Z","shell.execute_reply.started":"2021-11-12T17:35:31.773009Z","shell.execute_reply":"2021-11-12T17:54:21.557555Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"# Save Predictions","metadata":{}},{"cell_type":"code","source":"# Predict on Test Dataset\nPredictions = []\nfor i in range(len(trained_classifiers)):\n    trained_classifier = trained_classifiers[i]\n    y_pred = trained_classifier.predict(test_data_pts[i])\n    Predictions.append([int(round(yp, 0)) for yp in list(y_pred)])","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:54:21.560413Z","iopub.execute_input":"2021-11-12T17:54:21.560682Z","iopub.status.idle":"2021-11-12T17:54:37.028694Z","shell.execute_reply.started":"2021-11-12T17:54:21.560649Z","shell.execute_reply":"2021-11-12T17:54:37.027742Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# Save Predictions\npreds_save_path = \"./submission.csv\"\nSaveContestPredictions(Predictions, preds_save_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T17:54:37.030306Z","iopub.execute_input":"2021-11-12T17:54:37.030669Z","iopub.status.idle":"2021-11-12T17:54:37.042482Z","shell.execute_reply.started":"2021-11-12T17:54:37.030622Z","shell.execute_reply":"2021-11-12T17:54:37.041507Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}