%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[addpoints,12pt,solution]{exam}
\printanswers
\usepackage{amsmath,amssymb}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{paralist}
\usepackage{psfrag}
\usepackage{fullpage}
\usepackage{fancybox}
\usepackage{ifthen}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{marvosym}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage[ruled,vlined,noresetcount]{algorithm2e}
\usepackage{enumitem}
\marksnotpoints


\begin{document}


\hrule
\vspace{1mm}
\noindent 
\begin{center}
{\Large CS5691: Pattern Recognition and Machine Learning} \\
{\large Assignment \#1}
\end{center}
\vspace{1mm}
\noindent 
\small{\textbf{Topics:} Regression, Classification, Density Estimation \hfill \textbf{Deadline:} 04 Oct 2021, 11:55 PM} \\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Enter name and roll number here
\noindent {\bf Teammate 1:} (your name here) \hfill {\bf Roll number:} CS21YZZZ \\
\noindent {\bf Teammate 2:} (your name here) \hfill {\bf Roll number:} CS21YZZZ \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{2mm}
\hrule

{\small

\begin{itemize}\itemsep0mm

\item This assignment has to be completed in teams of 2. Collaborations outside the team are strictly prohibited.

\item Be precise with your explanations. Unnecessary verbosity will be penalized.

\item Check the Moodle discussion forums regularly for updates regarding the assignment.

\item Type your solutions in the provided \LaTeX ~template file.

\item For coding questions you will be required to upload the code in a zipped file to Moodle as well as embed the result figures in your \LaTeX ~solutions.

\item Attach a \textbf{README} with your code submission which gives a brief overview of your approach and a single command-line instruction for each question to read the data and generate the test results and figures. 

\item We highly recommend using \texttt{Python 3.6+} and standard libraries like \texttt{numpy, Matplotlib, pandas}. You can choose to use your favourite programming language however the TAs will only be able to assist you with doubts related to Python. 

\item You are supposed to write your own algorithms, any library functions which implement these directly are strictly off the table. Using them will result in a straight zero on coding questions, \texttt{import} wisely!

\item \textbf{Please start early and clear all doubts ASAP.}

\item Please note that the TAs will \textbf{only} clarify doubts regarding problem statements. The TAs won't discuss any prospective solution or verify your solution or give hints.  

\item Post your doubt only on Moodle so everyone is on the same page.

\end{itemize}
}

\hrule

\vspace{3mm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% START HERE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{questions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question \textbf{[Regression]} You will implement linear regression as part of this question for the dataset provided. For each sub-question, you are expected to report the following - (i) plot of the best fit curve, (ii) equation of the best fit curve along with coefficients, (iii) value of final least squared error over the test data and (iv) scatter plot of model output vs expected output and for both train and test data. You can also generate a \texttt{.csv} file with your predictions on the test data which we should be able to reproduce when we run your command-line instruction. 

Note that you can only regress over the points in the train dataset and you are not supposed to fit a curve on the test dataset. Whatever solution you get for the train data, you have to use that to make predictions on the test data and report results.

\begin{enumerate}[label=(\alph*)]

\item (2 marks) Use standard linear regression to get the best fit curve. Vary the maximum degree term of the polynomial to arrive upon an optimal solution. 

\begin{solution}

\end{solution}

\item (1 mark) In the above problem, increase the maximum degree of the polynomial such that the curve overfits the data.

\begin{solution}

\end{solution}

\item (2 marks) Use ridge regression to reduce the overfit in the previous question, vary the value of lambda ($\lambda$) to arrive at the optimal value. Report the optimal $\lambda$ along with other deliverables previously mentioned.

\begin{solution}

\end{solution}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question \textbf{[Classification]} You will implement classification algorithms that you have seen in class as part of this question. You will be provided train and test data as before, of which you are only supposed to use the train data to come up with a classifier which you will use to just make predictions on the test data. For each sub-question below, plot the test data along with your classification boundary and report confusion matrices on both train and test data. Again, your code should generate a \texttt{.csv} file with your predictions on the test data as before.

\begin{enumerate}[label=(\alph*)]

\item (2 marks) Implement the Preceptron learning algorithm with starting weights as $\mathbf{w} = [0, 1]^T$ for $\mathbf{x} = [x, y]^T$ and with a margin of 1. 

\begin{solution}

\end{solution}

\item (1 mark) Calculate (code it up!) a Discriminant Function for the two classes assuming Normal distribution when the covraiance matrices for both the classes are equal and $C_1 = C_2 = \sigma^2 I$ for some $\sigma$.

\begin{solution}

\end{solution}

\item (1 mark) Calculate a Discriminant Function for the two classes assuming Normal distribution when both $C_1$ and $C_2$ are full matrices and $C_1 = C_2$.

\begin{solution}

\end{solution}

\item (1 mark) Calculate a Discriminant Function for the two classes assuming Normal distribution when both $C_1$ and $C_2$ are full matrices and $C_1 \neq C_2$.

\begin{solution}

\end{solution}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question \textbf{[Probability]} In this question, you are required to verify if the following probability mass functions over their respective supports $S$ follow the following properties:

\begin{enumerate}
    \item $P(X=x) \geq 0 \ \ \forall x \in S$, and
    \item $\sum_{x \in S} P(X=x) = 1$.
\end{enumerate}

In addition, find the expectation, $\mathbb{E}(X)$ and variance, $Var(X)$ in the following cases.
\begin{parts}

    \part[2]  A discrete random variable X is said to have a Geometric distribution, with parameter $p \in (0,1]$ over the support $S = \{1,2,3,\dots\}$ if it has the following probability mass function:
    $$P(X=x) = (1-p)^xp$$
    
    \begin{solution}
    
    \end{solution}
    
    \part[2] A discrete random variable X is said to have a Poisson distribution, with parameter $\lambda > 0$ over the support $S = \{0,1,2,\dots\}$ if it has the following probability mass function:
    $$P(X=x) = \frac{\lambda^x e^{-\lambda}}{x!}$$
    
    \begin{solution}
    
    \end{solution}

\end{parts}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question \textbf{[Linear Regression]} Recall the closed form solution for linear regression that we derived in class, the following questions are a follow-up to the same.

\begin{parts}

\part[2] Say we have a dataset where every datapoint has a weight identified with it. Then we have the error function (sum of squares) given by 
\[
E(w)=\sum_{j=1}^{N} \dfrac{q_j(y_j - w^T x_j)^2}{2}
\]
where $q_j$ is the weight associated with each of the datapoints ($q_j > 0$). Derive the closed form solution for $w^{*}$.

\begin{solution}

\end{solution}

\part[1] We saw in class that the error function in case of ridge regression is given by:
\[
\dfrac{1}{2}\sum_{n=1}^{N}(t_n-w^{T}\phi(x_n))^2+\dfrac{\lambda}{2}w^{T}w
\]
Show that this error is minimized by :
\[
w^{*}=(\lambda I + \phi^{T}\phi)^{-1}\phi^{T}t
\]
Also show that $(\lambda I + \phi^{T}\phi)$ is invertible for any $\lambda > 0$.

\begin{solution}

\end{solution}

\part[1] Given 
\[
X=\begin{bmatrix}
-2 &6\\-1 &3
\end{bmatrix}\quad y=\begin{bmatrix}
3\\-1
\end{bmatrix}
\]
Solve $X^{T}X w = X^{T}y$ such that the Euclidean norm of the solution $w^{*}$ is minimum. 

\begin{solution}

\end{solution}

\end{parts}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question[2] \textbf{[Naive Bayes]} For multiclass classification problems, $p(C_k | \mathbf{x})$ can be written as:
\begin{equation*}
    p(C_k | \mathbf{x}) = \frac{exp(a_k)}{\sum_j exp(a_j)}
\end{equation*}
where $a_k = \ln p(\mathbf{x}|C_k) p(C_k)$. The above form is called the normalized exponential or softmax function. Now, consider a $K$ class classification problem for which the feature vector $\mathbf{x}$ has $M$ components. Each component is a categorical variable and takes one of $L$ possible values. Let these components be represented using one-hot encoding. Let us also make the naive Bayes assumption that the features are independent given the class. Show that the quantities $a_k$ are linear functions of the components of $\mathbf{x}$.

\begin{solution}

\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question[2] \textbf{[Naive Bayes]} Consider a Gaussian Naive Bayes classifier for a dataset with single attribute x and two classes 0 and 1. The parameters of the Gaussian distributions are:
\begin{equation*}
p(x|y=0) \sim \mathcal{N}(0, 1/4)
\end{equation*}
\begin{equation*}
p(x|y=1) \sim \mathcal{N}(0, 1/2)
\end{equation*}
\begin{equation*}
P(y = 1) = 0.5
\end{equation*}
Find the decision boundary for this classifier if the loss matrix is
L = 
$ \begin{bmatrix}
0 & \sqrt{2} \\
1 & 0 
\end{bmatrix}
$

\begin{solution}

\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question \textbf{[MLE]} Let \textit{x} have an exponential density
\[ 
p(x|\theta) = \begin{cases} 
      \theta e^{-\theta x} & x \geq 0\\
      0 & \text{otherwise} \\
   \end{cases}
\]

\begin{parts}

\part[2] Plot $p(x|\theta)$ versus $x$ for $\theta = 1$. Plot $p(x|\theta)$ versus $\theta$, $(0 \leq \theta \leq 5),$ for $ x = 2$.

\begin{solution}

\end{solution}

\part[1] Suppose that $n$ samples $x_1,..., x_n$ are drawn independently according to $p(x|\theta)$. Give the maximum likelihood estimate for $\theta$.

\begin{solution}

\end{solution}

\part[2] On the graph generated with $\theta = 1$ in part (a), mark the maximum likelihood estimate $\hat{\theta}$ for large $n$. Write down your observations. 

\begin{solution}

\end{solution}

\end{parts}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\question[3] \textbf{[MLE]} Gamma distribution has a density function as follows
\[
f(x|\alpha, \lambda) = \frac{1}{\Gamma(\alpha)} \lambda^\alpha x^{\alpha - 1} e^{-\lambda x}, \hspace{10pt} \text{with} \hspace{10pt} 0 \leq x \le \infty \]
Suppose the parameter $\alpha$ is known, please find the MLE of $\lambda$ based on an i.i.d.  sample $X_1, ..., X_n$.

\begin{solution}

\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% THE END %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{questions}
\end{document}