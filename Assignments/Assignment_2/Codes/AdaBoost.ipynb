{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expensive-agreement",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "\n",
    "Let us implement the AdaBoost algorithm, to build a powerful emsemble classifier from a set of weaker classifiers. Our base classifier will be a decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-equivalent",
   "metadata": {},
   "source": [
    "The training algorithm we will implement is as follows. We have $N$ training datapoints and are creating an ensemble of $k$ classifiers.\n",
    "\n",
    "- Initialize the weights for all datapoints ($w_j = 1/N$ for $j=1,2,...N$)\n",
    "- For $i = 1$ to $k$\n",
    "    - Form training set $D_i$ by sampling $N$ tuples (with replacement) from the full training dataset. The sampling probability for a tuple $(x_j,y_j)$ should be given by its corresponding weight $w_j$.\n",
    "    - Use dataset $D_i$ to fit a decision stump $M_i$. You can use sklearn's DecisionTreeClassifier with max_depth=1 to fit a decision stump.\n",
    "    - Calculate the error rate for $M_i$ using the sum of the weights of the misclassified points.\n",
    "    $$err(M_i) = \\sum_{j=1}^N w_j * \\mathbb{1}\\{y_j \\ne M_i(x_j)\\}$$\n",
    "    - The weight of classifier $M_i$'s vote is computed as $\\alpha_i = 0.5*\\log(\\frac{1-err(M_i)}{err(M_i)})$\n",
    "    - Increase the weight of the misclassified training points, and decrease the weight of the correctly classified training points.\n",
    "    $$w_j \\leftarrow w_j * \\exp\\{- \\alpha_i * y_j * M_i(x_j)\\}$$\n",
    "    - Remember to normalize the weights so that they sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries / functions that you use in your solution\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_AdaBoost(X, y, k):\n",
    "    \n",
    "    classifiers = []\n",
    "    alphas = []\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    D = np.concatenate((X, y.reshape(-1, 1)), axis=1)\n",
    "    \n",
    "    # Initialize weights\n",
    "    W = np.ones(N) / N\n",
    "    \n",
    "    for i in range(k):\n",
    "        \n",
    "        # Sample from the dataset according to weights\n",
    "        sample = np.random.choice(N, size=N, replace=True, p=W)\n",
    "        X_sample = X[sample]\n",
    "        y_sample = y[sample]\n",
    "        \n",
    "        # Fit a decision stump\n",
    "        classifier = DecisionTreeClassifier(max_depth=1)\n",
    "        classifier.fit(X_sample, y_sample)\n",
    "        \n",
    "        # Calculate the error rate\n",
    "        y_pred = classifier.predict(X_sample)\n",
    "        error = np.sum(W[sample] * (y_sample != y_pred))\n",
    "        \n",
    "        # Calculate the weight of classifier's vote\n",
    "        alpha = 0.5 * np.log((1 - error) / error)\n",
    "        \n",
    "        # Increase the weight of misclassified points\n",
    "        W[sample] = W[sample] * np.exp(-alpha * y_sample * y_pred)\n",
    "\n",
    "        # Normalise Weights to sum to 1\n",
    "        W = W / np.sum(W)\n",
    "        \n",
    "        # Append your classifier to the list classifiers\n",
    "        classifiers.append(classifier)\n",
    "        \n",
    "        # Append your alpha to the list alphas\n",
    "        alphas.append(alpha)\n",
    "        \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # classifiers and alphas need of be of type <class 'list'>\n",
    "    return classifiers, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-delhi",
   "metadata": {},
   "source": [
    "To obtain predictions, the vote of each classifier $M_i$ is weighted by its corresponding coefficient $\\alpha_i$.\n",
    "\n",
    "$$y_i = \\text{sign}\\{\\sum_{i=1}^k \\alpha_i*M_i(x_i)\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_AdaBoost(X,classifiers, alphas):\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    N = X.shape[0]\n",
    "    y_pred = np.zeros((N, 1))\n",
    "\n",
    "    for i in range(len(classifiers)):\n",
    "        y_pred += alphas[i] * classifiers[i].predict(X)\n",
    "\n",
    "    y_pred = np.sign(y_pred)\n",
    "    y_pred[y_pred == 0] = -1\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # y_pred needs to be of type <class 'numpy.ndarray'>\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-installation",
   "metadata": {},
   "source": [
    "The below function will help you plot the decision surface given by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_AdaBoost(X, y, classifiers, alphas):\n",
    "    \n",
    "    # Get limits of x and y for plotting the decision surface\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    \n",
    "    # Get points at a distance of h between the above limits \n",
    "    h = .02    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    temp = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # Classify the all the points\n",
    "    P = predict_AdaBoost(temp, classifiers, alphas).reshape(yy.shape)\n",
    "    \n",
    "    # Plot the decision boundary and margin\n",
    "    plt.pcolormesh(xx, yy, P, cmap=plt.cm.coolwarm, shading='auto')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm,edgecolor='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-tourist",
   "metadata": {},
   "source": [
    "Load the given datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dataset Functions\n",
    "def ReadDataset_NoHeaders(path):\n",
    "    dataset = pd.read_csv(path, header=None)\n",
    "    return dataset\n",
    "\n",
    "def SaveDataset(dataset, path):\n",
    "    dataset.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fba9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATASET_PATH = DRIVE_PATH + \"MyDrive/PRML Assignments/Assignment 2/Datasets/Q2/\"\n",
    "# DATASET_PATH = DRIVE_PATH + \"PRML/IITM/Question1\"\n",
    "train_path_X = DATASET_PATH + \"X_train.csv\"\n",
    "train_path_Y = DATASET_PATH + \"y_train.csv\"\n",
    "test_path_X = DATASET_PATH + \"X_test.csv\"\n",
    "test_path_Y = DATASET_PATH + \"y_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "Dataset_train_X = ReadDataset_NoHeaders(train_path_X)\n",
    "Dataset_train_Y = ReadDataset_NoHeaders(train_path_Y)\n",
    "Dataset_test_X = ReadDataset_NoHeaders(test_path_X)\n",
    "Dataset_test_Y = ReadDataset_NoHeaders(test_path_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Dataset X:\", Dataset_train_X.shape)\n",
    "print(\"Train Dataset Y:\", Dataset_train_Y.shape)\n",
    "print(\"Test Dataset X:\", Dataset_test_X.shape)\n",
    "print(\"Test Dataset Y:\", Dataset_test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-garden",
   "metadata": {},
   "source": [
    "Plot the training data as a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Functions\n",
    "def PlotDataset(dataset, title=''):\n",
    "    X = dataset.iloc[:, 0]\n",
    "    Y = dataset.iloc[:, 1]\n",
    "    plt.scatter(X, Y)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def PlotLabelledDataset(dataset, title=''):\n",
    "    X = dataset[:, 0]\n",
    "    Y = dataset[:, 1]\n",
    "    C = dataset[:, 2]\n",
    "    classes_unique = np.unique(C)\n",
    "    for c in classes_unique:\n",
    "        x = X[C == c]\n",
    "        y = Y[C == c]\n",
    "        plt.scatter(x, y, label=c)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29c1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Points Scatter Plot\n",
    "print(\"Train Datapoints\")\n",
    "PlotDataset(Dataset_train_X, \"Train Datapoints\")\n",
    "\n",
    "# Scatter Plot with classes\n",
    "Dataset_train = np.zeros((Dataset_train_X.shape[0], 3))\n",
    "Dataset_train[:, :2] = Dataset_train_X\n",
    "Dataset_train[:, 2] = Dataset_train_Y.iloc[:, 0]\n",
    "print(\"Train Datapoints with Classes\")\n",
    "PlotLabelledDataset(Dataset_train, \"Train Datapoints with Classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-status",
   "metadata": {},
   "source": [
    "Use the train_AdaBoost function to train an AdaBoost model with k=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers, alphas = train_AdaBoost(Dataset_train_X, Dataset_train_Y, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-agriculture",
   "metadata": {},
   "source": [
    "Use the predict_AdaBoost function to make predictions on X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_AdaBoost(Dataset_train_X, classifiers, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-crowd",
   "metadata": {},
   "source": [
    "Use the plot_AdaBoost function to plot the learnt decision surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_AdaBoost(Dataset_train_X, y_pred, classifiers, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-ceremony",
   "metadata": {},
   "source": [
    "Compute the accuracy of the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(y_pred == Dataset_train_Y.iloc[:, 0]) / Dataset_train_X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-attribute",
   "metadata": {},
   "source": [
    "Use the train_AdaBoost function to train an AdaBoost model with k=100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-concern",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bearing-boring",
   "metadata": {},
   "source": [
    "Use the predict_AdaBoost function to make predictions on X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-holly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "floral-conference",
   "metadata": {},
   "source": [
    "Use the plot_AdaBoost function to plot the learnt decision surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-syria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "equal-share",
   "metadata": {},
   "source": [
    "Compute the accuracy of the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-membrane",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
